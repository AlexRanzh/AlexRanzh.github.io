<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Introduction to Machine Learning</title>
      <link href="/2019/04/16/Introduction%20to%20Machine%20Learning/"/>
      <url>/2019/04/16/Introduction%20to%20Machine%20Learning/</url>
      
        <content type="html"><![CDATA[<p>This note is based on 2019 Spring COMPSCI189/289A course at University of California, Berkeley by Jonathan Shewchuk.</p><p>#Supervised Learning</p><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><h3 id="example-1-classify-digit-1-and-7"><a href="#example-1-classify-digit-1-and-7" class="headerlink" title="example 1: classify digit 1 and 7"></a>example 1: classify digit 1 and 7</h3><ol><li>$N\times N$ pixels matrices</li><li>flatten into vector</li><li>create a classifier for $N\times N$ space<br>Note: Decision Boundary is a hyperplane</li></ol><h3 id="example-2-Bank-loan-default-prediction"><a href="#example-2-Bank-loan-default-prediction" class="headerlink" title="example 2: Bank loan default prediction"></a>example 2: Bank loan default prediction</h3><p>See notes for M3S17<a href>Quantitative Methods for Finance</a></p><h3 id="Feature-Independent-Variables-Predictor-Variables"><a href="#Feature-Independent-Variables-Predictor-Variables" class="headerlink" title="Feature/Independent Variables/ Predictor Variables"></a>Feature/Independent Variables/ Predictor Variables</h3><p>?</p><h3 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h3><p>The model is shaped to specifically to one certain data set so not predictive to new data.</p><p>When the test error worse coz the classifier becomes too sensitive to outliers or to other spurious/untrue patterns.</p><p>Sinuous decision boundaries that fit the sample points so well that it do not classify future points well.</p><h4 id="Quantify-Overfitting"><a href="#Quantify-Overfitting" class="headerlink" title="Quantify Overfitting"></a>Quantify Overfitting</h4><h3 id="Decision-Boundary"><a href="#Decision-Boundary" class="headerlink" title="Decision Boundary"></a>Decision Boundary</h3><p>The boundary chosen by classifier to separate items in class from those are not.</p><h3 id="Decision-Function"><a href="#Decision-Function" class="headerlink" title="Decision Function"></a>Decision Function</h3><p>A function $f(x)$ that maps a sample point to a scalar value that<br>$$<br>f(x)&gt;0 if x \in class C<br>f(x)\leq if x not \in class C<br>$$</p><p>For these decision function, the decision boundary is $f(x)=0$, <strong>usually</strong> a (d-1) dimensional surface in $R^d$.</p><h3 id="Isosurface-iso-contours"><a href="#Isosurface-iso-contours" class="headerlink" title="Isosurface/iso contours"></a>Isosurface/iso contours</h3><p>A isosurface for function $f$ is ${x: f(x)=0}$, 0 is isovalue here.<br>Note: ‚Äòiso-‚Äò prefix means ‚Äòequal-‚Äò</p><h3 id="Linear-classifier"><a href="#Linear-classifier" class="headerlink" title="Linear classifier"></a>Linear classifier</h3><p>The decision boundary is a line/plane<br>Usually a linear decision function.</p><p>$$<br>x=(x_1,x_2,‚Ä¶,x_5)^T</p><p>$$</p><h3 id="Conventions"><a href="#Conventions" class="headerlink" title="Conventions:"></a>Conventions:</h3><p>Uppercase roman: matrix, random variables, set<br>Lowercase roman: vector<br>Greek: scalar<br>Other scalar:</p><ul><li>n, number of sample points</li><li>d, number of features</li><li>i,j,k, integer indices<br>Function: f(), s(),‚Ä¶</li></ul><h3 id="Norms"><a href="#Norms" class="headerlink" title="Norms"></a>Norms</h3><ul><li>Euclidean norms</li><li>Normalize a vector: $\frac{x}{|x|}$</li><li>dot product:<ul><li>length:$|x|=\sum x_i y_i$</li><li>angle: $cos(\theta)=\frac{x\dot y}{|x||y|}$</li></ul></li></ul><h3 id="hyperplane"><a href="#hyperplane" class="headerlink" title="hyperplane"></a>hyperplane</h3><p>Given a decision function $f(x)=w\dot x+\alpha$ is $H={x: w\dot x =-\alpha }$<br>The set H is a <strong>hyperplane</strong>.</p><p><strong>property</strong><br>For any x, y on H, $w\dot(y-x)=0$</p><p><strong>normal vector</strong> w</p><p><strong>signed distance</strong>$w\dot x +\alpha$, $w$ is unit vector<br>i.e. positive on one side of H, negative on the other side</p><p>Note: the distance from H to the origin is $\alpha$.<br>Note2: $\alpha =0$ iff H passes through origin</p><h3 id="weights"><a href="#weights" class="headerlink" title="weights"></a>weights</h3><p>coefficients in $w$ and$\alpha$ are called <strong>weights</strong> or <strong>regression coefficients</strong></p><h3 id="Linearly-separable"><a href="#Linearly-separable" class="headerlink" title="Linearly separable"></a>Linearly separable</h3><p>the input data is <strong>linearly separable</strong> if there exists a hyperplane that separates all the sample planes in C from those not in C.</p><h3 id="Centriod-classifier"><a href="#Centriod-classifier" class="headerlink" title="Centriod classifier"></a>Centriod classifier</h3><p>computer mean $\mu_c$ of all vectors in class C and meam $\mu_x$ of al vectors NOT in class C.</p><ul><li>Decision function:<br>$f(x)=(\mu_c-\mu_x)\dot x-(\mu_c-\mu_x)\dot \frac{(\mu_c-\mu_x)}{2}$<br>$(\mu_c-\mu_x)$ is normal vector<br>$\frac{(\mu_c-\mu_x)}{2}$ is midpoint between$\mu_c, \mu_x$<br>so the decision boundary is the hyperplane that bisects \bar{\mu_c\mu_x}</li><li>good at: classify with samples from two gaussian/normal distributions, especially when sample size is large</li></ul><h3 id="Perceptron-Algorithm"><a href="#Perceptron-Algorithm" class="headerlink" title="Perceptron Algorithm"></a>Perceptron Algorithm</h3><blockquote><p>Slow but correct for linearly separable points.<br>Uses a <strong>numerical optimisation</strong> algorithm, namely the <strong>gradient decent</strong>.</p></blockquote><p>Sample points $X_1,X_2,‚Ä¶,X_n$.<br>For each sample point, $y_i = 1(\in C)\ or\ -1(\notin C)$</p><p>For simplicity, assume the decision boundaries pass through the origin.</p><h2 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h2><p>#Unsupervised Learning</p><h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><h2 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h2><h1 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a>Validation</h1><p>Hold back a subset of training data for future test use‚Äìvalidation set(to tune hyperparameters/choose model).</p><p>test set: final evaluation</p><ol><li>train a classifier multiple times, with different model/hyperparameter</li><li>test it on NEW data</li><li>choose the setting that gives the best validation result<br>?why: we want the model to be working in general data. In most cases(knn for example),input used data will always give the right output, which is not valuable for our evaluation of the model.</li></ol><h2 id="Types-of-error"><a href="#Types-of-error" class="headerlink" title="Types of error"></a>Types of error</h2><ol><li>training set error: fraction of training images not classified correctly</li><li>test set error:<br>fraction of misclassifying new data</li></ol><h1 id="outlier"><a href="#outlier" class="headerlink" title="outlier:"></a>outlier:</h1><p>points are atypical</p><h1 id="hyperparameters"><a href="#hyperparameters" class="headerlink" title="hyperparameters"></a>hyperparameters</h1><p>~ control overfitting/underfitting(e.g. k in knn)</p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/04/14/hello-world/"/>
      <url>/2019/04/14/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><p><img src="/photos/1.png" alt></p><a id="more"></a><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p><img src="https://itimetraveler.github.io/hexo-theme-hiker/2016/10/24/Hiker%E4%B8%BB%E9%A2%98%E9%A2%84%E8%A7%88/homepage-index.png" alt></p><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Quantitative Methods in Retail Finance Note 4</title>
      <link href="/2019/04/14/Quantitative%20Methods%20in%20Retail%20Finance%20Note4/"/>
      <url>/2019/04/14/Quantitative%20Methods%20in%20Retail%20Finance%20Note4/</url>
      
        <content type="html"><![CDATA[<p>This notes follows materials from the slides for M3S17 at Imperial College London Mathematics Department by Dr. Tony Bellotti. Questions mentioned are referring to past exam papers of M3S17 at Imperial College London.</p><p>#Chapter 3: Markov Transition Model</p><h2 id="Markov-Transition-Models"><a href="#Markov-Transition-Models" class="headerlink" title="Markov Transition Models"></a>Markov Transition Models</h2><h3 id="Why-Markov-Chain-Model"><a href="#Why-Markov-Chain-Model" class="headerlink" title="Why Markov Chain Model?"></a>Why Markov Chain Model?</h3><p>This model allow us to track model <strong>changes in the state</strong> of an account over time.<br>e.g. Tracking credit card use.</p><h3 id="First-order-Markov-Transition-Model"><a href="#First-order-Markov-Transition-Model" class="headerlink" title="First-order Markov Transition Model"></a>First-order Markov Transition Model</h3><ul><li>Let $(X_1, X_2, ‚Ä¶, X_n)$ be a sequence of discrete random variables that take value from ${1,2,‚Ä¶,K}$ with K fixed.</li><li><strong>first-order finite value Markov chain</strong><br>The sequence is said to be a <em>first-order finite value Markov chain</em> if<br>$$<br>P(X_{n+1}=j|X_0=x_0,X_1=x_1,X_2=x_2,‚Ä¶,X_{n-1}=x_{n-1},X_n = i) = P(X_{n+1}=j|X_n = i)<br>$$<br>for all $n, x_0,x_1, ‚Ä¶, x_{n-1}$ and $i,j$ such that $1\leq i,j \leq K$.</li><li><strong>transition probability</strong><br><em>Transition probability</em> $p_n$ is defined as<br>$$<br>p_{n}(i,j)=P(X_n=j|X_{n-1}=i)<br>$$</li><li><strong>transition matrix</strong><br><em>Transition matrix</em> $P$ is defined as a $K \times K$ matrix such that<br>$$<br>P_n[i,j] = p_{n}(i,j)<br>$$</li><li><strong>structural zeroes</strong><br>If it is certain that no transition will be made from i to j in the $n$th period, then set $p_n(i,j)=0$, this is called a <em>structural zero</em>.</li></ul><p>###Propagation of Markov Chain<br>If we know the transition matrix from period $n-r$ to $n$, i.e. $P_r,P_{r+1},‚Ä¶,P_{n-1}$ are known, then we also be able to know the state in the $n$th period:<br>$$<br>Prob(X_{n}=j|X_{n-r}=i) = P_{n-r+1}P_{n-r+2}‚Ä¶P_{n}[i,j]<br>$$<br>for any $1\leq r \leq n$</p><p>Forcasting from state 0:<br>$$<br>Prob(X_{n}=j|X_{0}=i) = P_{1}P_{2}‚Ä¶P_{n}[i,j]<br>$$<br>let $\pi_{i}$ be the marginal distribution for $X_n$:<br>$$\pi_n = (P(X_n = 1),P(X_n = 2),‚Ä¶,P(X_n = K))^T$$</p><p>Then,<br>$$\pi_n = \pi_0(P_1 P_2 ‚Ä¶ P_n)$$</p><h3 id="Stationary-Markov-Chain"><a href="#Stationary-Markov-Chain" class="headerlink" title="Stationary Markov Chain"></a>Stationary Markov Chain</h3><ul><li><strong>stationary</strong><br>A Markov chain is <em>stationary</em> if $p_n(i,j)=p(i,j)$ for all $<strong>n</strong>,\ and\ i,j\ where 1\leq i, j \leq K$, for some transition probability $p$.</li><li><strong>stationary distribution</strong><br>A <em>stationary distribution</em> for transition matrix is a distribution $\pi^\star = \pi^\star P$.<blockquote><p>In practice, most Markov chains converge (with ùëõ) to a stationary<br>distribution.<br>(Markov chains which have a periodicity in state change do not necessarily converge, but we do not cover this material in this course).</p></blockquote></li></ul><h2 id="Estimation"><a href="#Estimation" class="headerlink" title="Estimation"></a>Estimation</h2><p>The aim this section, given some data, is to estimate $\mathbf{\theta}=(\theta_{ij})^{K}<em>{i,j=1}$ where $\theta</em>{ij}=p(i,j)$.<br>Maximum likelihood estimator for $\mathbf{\theta}$:<br>$$<br>\hat\theta_{ij}=\hat p(i,j)=\frac{n_{ij}}{\sum_{l=1}^{n}n_{il}}<br>$$<br>where $n_{ij}=|{t:x_{t-1}=i,x_{t}=j,\ for \ t\in {1,2,‚Ä¶,n}}|$, i.e. total number of observations that move from state $i$ to state $j$ during period 1 to n.<br><strong>proof</strong></p><blockquote><p>2017Q1<br>(c) Derive the maximum likelihood estimator for the first order finite-valued Markov transition model, assuming stationarity.</p></blockquote><ol><li>likelihood function</li></ol><p>$P(X_0=x_0,X_1=x_1,‚Ä¶,X_n=x_n)$<br>$=\prod_{t=1}^{n} P(X_{t}=x_t|X_{t-1}=x_{t-1},X_{t-2}=x_{t-2},‚Ä¶,X_{1}=x_{1})P(X_0=x_0)$<br>$=\prod_{t=1}^{n} P(X_{t}=x_t|X_{t-1}=x_{t-1})P(X_0=x_0)(Chain rule)$<br>$=P(X_0=x_0)\prod_{t=1}^{n} P(X_{t}=x_t|X_{t-1}=x_{t-1})$<br>$=P(X_0=x_0)\prod_{i=1}^{n}\prod_{j=1}^{n}[p(i,j)]^{n_{ij}}$<br>$=P(X_0=x_0)\prod_{i=1}^{n}\prod_{j=1}^{n}[\theta_{ij}]^{n_{ij}}$<br>$$<br>where\ n_{ij}=|{t:x_t=x_t,X_{t-1}=x_{t-1},\ for\ t\in {1,‚Ä¶,n}}|<br>$$</p><ol start="2"><li>log likelihood function</li><li>differentiate and find MLE</li><li>adjust to constraints  </li></ol><h2 id="Testing-First-order-Assumptions"><a href="#Testing-First-order-Assumptions" class="headerlink" title="Testing First-order Assumptions"></a>Testing First-order Assumptions</h2><p>In order to test if first order markov chain is a suitable assumption in each occasion,<br>we use chi-square hypothesis test.</p><h3 id="second-order-Markov-chain"><a href="#second-order-Markov-chain" class="headerlink" title="second order Markov chain"></a>second order Markov chain</h3><p>A sequence (x_1,..,x_n) is a <em>second order Markov chain</em> if<br>$P(x_n+1=j|X_0=x_0,‚Ä¶,X_{n-1}=k,X_{n}=i)=P(x_n+1=j|X_{n-1}=k,X_{n}=i)$</p><p>The <strong>transition probability</strong><br>$p_n(k,i,j)=P(X_n=i, X_{n-1}=k)$</p><p>The <strong>MLE for a second-Markov chain</strong> can similarly be proved as:<br>$\hat p(k,i,j)=\hat\theta_{kij}=\frac{n_kij}{mki}$<br>where $n_{kij}=|{t:x_{t-2}=k,x_{t-1}=i,x_{t}=j\ for\ t \in {2,‚Ä¶,n}}|,<br>m_{ki}=|{t:x_{t-2}=k,x_{t-1}=i\ for\ t \in {2,‚Ä¶,n}}|=\sum_{l=i}^{n}n_{kil}$</p><ol><li><p><strong>$H_{0}$</strong><br>Null hypothesis:<br>$P(1,i,j)=P(2,i,j)=‚Ä¶=P(K,i,j)=P(i,j)$<br>where $P(k,i,j)$ is second-order Markov transition probability from state k to i then to j.</p></li><li><p><strong>$n_k$</strong><br>$n_k=|{t:x_t=k\ for\ t \in {1,..,n}}|$<br>i.e. The observed number of times for each states</p></li><li><p><strong>$O_{kj}$</strong><br>The observed number of times state k follwed by i and then j<br>$O_{kj}=n{kij}=m_{ki}\hat p(i,j)$</p></li><li><p><strong>$E_{kj}$</strong><br>The expected number of times state $k$ is followed by state $i$ and then $j$, <strong>given null hypothesis</strong> is:<br>$E_{kj}=n_{k}\hat p(k,i)\hat p(i,j)\approx m_{ki}\hat p(i,j)$<br>becuase $\hat p(k,i)=\frac{n_{ki}}{\sum_{l=1}^{K}n_{kl}}\approx \frac{m_{ki}}{n_k}$</p></li><li><p><strong>Pearson Chi_square test</strong><br>$$<br>\chi^2 = \sum_{k\in S_1}\sum_{k\in S_2}\frac{(O_{kj}-E{kj})^2}{E_{kj}}<br>$$</p></li></ol><ul><li>S1 is the set of states which do not have a structural zero moving to state $i$</li><li>S2 is the set of states which do not have a structural zero moving from state $i$</li></ul><ol start="6"><li><strong>Degree of freedom</strong><br>$df = (K-1-z_{1})(K-1-z_{2})$</li></ol><ul><li>$z_1$: the number of structural zeroes in the transition from $k$ to $i$.</li><li>$z_2$: the number of structural zeroes in the transition from $i$ to $j$.</li></ul><h2 id="Roll-rate-Model"><a href="#Roll-rate-Model" class="headerlink" title="Roll-rate Model"></a>Roll-rate Model</h2>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
          <category> Math Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Credit Modelling </tag>
            
            <tag> Markov Transition Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Quantitative Methods in Retail Finance Note 3</title>
      <link href="/2019/04/14/Quantitative%20Methods%20in%20Retail%20Finance%20Note3/"/>
      <url>/2019/04/14/Quantitative%20Methods%20in%20Retail%20Finance%20Note3/</url>
      
        <content type="html"><![CDATA[<p>This notes follows materials from the slides for M3S17 at Imperial College London Mathematics Department by Dr. Tony Bellotti. Questions mentioned are referring to past exam papers of M3S17 at Imperial College London.</p><h1 id="Chapter-2-Survival-Models-for-Credit-Risk"><a href="#Chapter-2-Survival-Models-for-Credit-Risk" class="headerlink" title="Chapter 2: Survival Models for Credit Risk"></a>Chapter 2: Survival Models for Credit Risk</h1>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
          <category> Math Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Survival Model </tag>
            
            <tag> Credit Modelling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Quantitative Methods in Retail Finance Note 2</title>
      <link href="/2019/04/14/Quantitative%20Methods%20in%20Retail%20Finance%20Note2/"/>
      <url>/2019/04/14/Quantitative%20Methods%20in%20Retail%20Finance%20Note2/</url>
      
        <content type="html"><![CDATA[<p>This notes follows materials from the slides for M3S17 at Imperial College London Mathematics Department by Dr. Tony Bellotti. Questions mentioned are referring to past exam papers of M3S17 at Imperial College London.</p><h1 id="Chapter-8-Artificial-Neural-Networks-in-Fraud-detection"><a href="#Chapter-8-Artificial-Neural-Networks-in-Fraud-detection" class="headerlink" title="Chapter 8: Artificial Neural Networks in Fraud detection"></a>Chapter 8: Artificial Neural Networks in Fraud detection</h1><p>##</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
          <category> Math Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ANN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Quantitative Method in Retail Finance Note 1</title>
      <link href="/2019/04/13/Quantitative%20Methods%20in%20Retail%20Finance%20Notes1/"/>
      <url>/2019/04/13/Quantitative%20Methods%20in%20Retail%20Finance%20Notes1/</url>
      
        <content type="html"><![CDATA[<p>This notes follows materials from the slides for M3S17 at Imperial College London Mathematics Department by Dr. Tony Bellotti. Questions mentioned are referring to past exam papers of M3S17 at Imperial College London.</p><h1 id="Chapter-7-Fraud-Detection-in-Retail-Credit"><a href="#Chapter-7-Fraud-Detection-in-Retail-Credit" class="headerlink" title="Chapter 7: Fraud Detection in Retail Credit"></a>Chapter 7: Fraud Detection in Retail Credit</h1><h2 id="Types-of-Fraud"><a href="#Types-of-Fraud" class="headerlink" title="Types of Fraud:"></a>Types of Fraud:</h2><ul><li>Theft</li><li>Counterfeit</li><li>Application</li><li>Card-not-present/online</li></ul><h2 id="Automated-Fraud-Detection"><a href="#Automated-Fraud-Detection" class="headerlink" title="Automated Fraud Detection"></a>Automated Fraud Detection</h2><p>This essentially is a classification problem, legitimate transaction Y=1, illegal Y=0.</p><p><strong>Special problems for fraud detection (that distinguish it from other classification problems)</strong></p><blockquote><p>2017 Q3(a) What special characteristics does fraud detection have, considered as a classification problem?<br>‚àó Need to process millions of transactions in real time.<br>‚àó Highly imbalanced classification problem: ratio of fraudulent to legitimate<br>transactions is typically very small.<br>‚àó Nature of fraud is reflexive. That is, fraudsters adapt to the detection methods applied by banks to stop them.</p></blockquote><ol><li>High volume data.</li><li>Data highly imbalanced.</li><li>Reflexive nature.</li></ol><p><em>difference of fraud detection problem from application model?</em><br>  Do not need to explain the model, so nonlinear complicated models are allowed.</p><h2 id="Business-Rule-method"><a href="#Business-Rule-method" class="headerlink" title="Business Rule method"></a>Business Rule method</h2><blockquote><p>For example, the card is used abroad<br>and it had not been used in the past year in the country<br>and the card holder did not tell the bank they will be abroad</p></blockquote><h2 id="Predictive-Model-method"><a href="#Predictive-Model-method" class="headerlink" title="Predictive Model method"></a>Predictive Model method</h2><p>Construct a two-class classifier - a fraud scorecard.</p><ul><li>the higher the score, the more trustworthy the account is.</li><li>classifier model with function $f$: $\hat{y} = f(x,\theta)$, and estimate $\theta$ with data x.</li><li>filter out some legitimate transaction to balance training data set. e.g. discard small amount transactions, regular transactions and inactive accounts.<br><strong>past research have shown that a linear model is not enough, non-linear classifiers like ANN works better.</strong></li><li>This method works well in detecting known type fraudulent transactions, but not able to deal with new fraud types.</li></ul><h2 id="Anomaly-Detection-method"><a href="#Anomaly-Detection-method" class="headerlink" title="Anomaly Detection method"></a>Anomaly Detection method</h2><p>Another method is to model only the legal transactions and detect any abnormal new transactions.</p><h3 id="Comparing-with-predictive-modelling"><a href="#Comparing-with-predictive-modelling" class="headerlink" title="Comparing with predictive modelling:"></a>Comparing with predictive modelling:</h3><p>  pros:</p><ol><li>it can detect new types of fraud</li><li>it do not worry about the imbalanced data(coz 1)<br>con:</li><li>it is not as sensible to transactions that are similar to legitimate ones</li></ol><h3 id="One-class-classifier-only-model-legitimate-pdf-over-predictor-variables"><a href="#One-class-classifier-only-model-legitimate-pdf-over-predictor-variables" class="headerlink" title="One-class classifier only model legitimate pdf over predictor variables"></a>One-class classifier only model legitimate pdf over predictor variables</h3><blockquote><p>2017Q3(b) What are the key differences between one-class and two-class classifiers for fraud?<br>‚àó Two-class classifiers are modelling the difference between fraudulent and legitimate transactions.<br>‚àó One-class classifiers just model distribution of legitimate transactions.<br>‚àó One-class classifiers are well-suited to deal with the reflexivity of fraud, since they do not explicitly model the existing pattern of fraud.</p></blockquote><h3 id="Steps-for-Anomaly-Detection"><a href="#Steps-for-Anomaly-Detection" class="headerlink" title="Steps for Anomaly Detection:"></a>Steps for Anomaly Detection:</h3><ol><li>drop out abnormal transactions: fraudulent transactions, errors, genuine but outliers</li><li>Let $S = (\mathbf{x_{1}},\mathbf{x_{2}},‚Ä¶,\mathbf{x_{n}})$ be a training sequence of legitimate transactions.</li><li>estimate $f(\mathbf{x}|S,\gamma)$ where $\gamma$ is an estimation parameter.</li><li>A classification decision is made with a new observation $\mathbf{x_{new}}$ at:<br>$$<br>\hat(y) = I(f(\mathbf{x_{new}}|S,\gamma)&gt;\theta)<br>$$<br>where $\theta$ is a threshold in probability.<blockquote><p>2017Q3(e) Which of these will register a fraud alert, based on your result in part (d)?<br>ANS: T2 and T3 have density less than Œ∏ = 0.05 and so will register a fraud alert.</p></blockquote></li></ol><p><em>The threshold $\theta$ is manually set based on the (sensible) strategy of controlling the fraction $\epsilon$ of legitimate cases to be classified as anomalous, based on training data.</em><br>‚Äî- This is constrained by how many false alert is generated/business resource can be input to followups.</p><blockquote><p>2017 Q3(d) Given a false alarm rate Œµ = 1/8, and based only on this data, what is the value of Œ∏?<br>ANS:<br>Use formula $$<br>max \theta s.t. \sum^{n}_{i=1}I(f(x_i) &gt; \theta) \leq n(1 ‚àí \epsilon)$$<br>where $$n(1 ‚àí \epsilon) = 24 √ó 7/8 = 21$$<br>Therefore, count lowest (24 ‚àí 21) = 3 densities from the table gives $\theta$ = 0.05.</p></blockquote><p>i.e.<br>$$<br>min\theta s.t.  \sum_{i=1}^{n}I(f(\mathbf{x}_{i}|S,\gamma)&gt;\theta)=floor(n\epsilon)<br>$$</p><h3 id="Kernel-Density-Estimator"><a href="#Kernel-Density-Estimator" class="headerlink" title="Kernel Density Estimator"></a>Kernel Density Estimator</h3><p>Crude empirical density:<br>$$<br>\hat{F_{EMP}}= \frac{1}{n}\sum_{i=1}^{n}I(x_{i}\leqslant x)<br>$$</p><p>kernel density function:</p><blockquote><p>2017Q3(c) Describe Parzen density estimation.</p></blockquote><p>$$<br>\hat{f(x)} = \frac{1}{nh^m}\sum_{i=1}^{n} K(\frac{x-x_{i}}{h})<br>$$s.t.</p><ol><li>$K(z) = K(-z)$</li><li>$\int K(z) dz = 1$</li><li>$\mathbf{x_1}, \mathbf{x_2}, ‚Ä¶, \mathbf{x_n}$ is observations with dimension m</li><li>h is bandwidth constant</li></ol><h3 id="Available-data"><a href="#Available-data" class="headerlink" title="Available data"></a>Available data</h3><ul><li>Account Data</li><li>Transaction Data</li><li>Personal Data</li><li>Location data</li></ul><h2 id="Social-Network-Analysis"><a href="#Social-Network-Analysis" class="headerlink" title="Social Network Analysis"></a>Social Network Analysis</h2><p>Using weight $w_{ij}$ to estimate <strong>association feature/degree of connectedness to fraud people</strong> $F_{i}$ and hence have fraud propensity from it.</p><blockquote><p>2017Q3(f) Describe in words what Fi is measuring?<br>(f) Variable $F_{i}$ measures the degree of connectedness to other people committing fraud.</p></blockquote><h3 id="Formula"><a href="#Formula" class="headerlink" title="Formula:"></a>Formula:</h3><p>$$<br>F_{i} = \frac{1}{\sum_{i=1}^{n}w_{ij}} \sum_{j=1}^{n} w_{ij}p_{j}<br>$$</p><blockquote><p>2017Q3(g) What range of values can Fi take?<br>$0\leq F_i \leq 1$</p></blockquote><blockquote><p>2017Q3(h) Calculate the value of Fi for T4 (node 3)?<br>Use formula in the question to get (4 + 1)/(2 + 4 + 1 + 1) = 5/8 = 0.625.</p></blockquote><blockquote><p><strong>2017Q3(i) How are values of Fi broadly affecting the density?</strong><br>(i)<br>Low values of $F_i$ are inducing a small <strong>increase</strong> in density.<br>High values of $F_i$ are inducing a large <strong>decrease</strong> in density.<br><strong>Low density means default</strong></p></blockquote><h2 id="Model-Evaluation"><a href="#Model-Evaluation" class="headerlink" title="Model Evaluation"></a>Model Evaluation</h2><p>Special performance evaluation for this problem:</p><ol><li>proportion fraud detected</li><li>keep low false alarm rate, not to upset customer</li><li>cost monitoring automated fraud alert</li></ol><h3 id="Receiver-operating-characteristics-ROC"><a href="#Receiver-operating-characteristics-ROC" class="headerlink" title="Receiver-operating characteristics (ROC)"></a>Receiver-operating characteristics (ROC)</h3><p>Two CDF measures:<br>$$y = {0,1}  F_{y}(c)=P(S\leq c|Y=y)$$</p><ul><li>y=0: given a threshold c, probability of fraud transaction that identified as fraud)</li><li>y=1: given a threshold c, probability of false alert generated</li></ul><h3 id="Area-under-ROC-curve-AUC"><a href="#Area-under-ROC-curve-AUC" class="headerlink" title="Area under ROC curve(AUC)"></a>Area under ROC curve(AUC)</h3><p>$$<br>A = \int F_{1}(c)F_{0}‚Äô(c)dc<br>$$</p><h3 id="Precision-Recall-and-alarm-rate"><a href="#Precision-Recall-and-alarm-rate" class="headerlink" title="Precision, Recall and alarm rate"></a>Precision, Recall and alarm rate</h3><ul><li>$F(c) = P(S\leq c)$</li><li>$p_{0} = P(Y=0)$</li><li>Recall: $F_{0}(c)$, the proportion of fraud transactions that are detected</li><li>Precision: the proportion alerts that are actually fraud<br>$P(Y=0|S\leq c)=\frac{F_{0}(c)p_0}{F(c)}$</li><li>Alarm rate: $F(c)= p_0 \frac{recall}{precision}$, unconditional probability of alarm<br><strong>Monitoring cost increases linear with alarm rate</strong></li></ul><h3 id="Precision-recall-PR-curve"><a href="#Precision-recall-PR-curve" class="headerlink" title="Precision-recall (PR) curve"></a>Precision-recall (PR) curve</h3><p>properties:</p><ul><li>typical decreasing</li><li>best: 1,1</li><li>worst: $F(c)=F_0 (c)$, horizontal line with $precision =p_0$</li></ul>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
          <category> Math Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ANN </tag>
            
            <tag> Credit modelling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DS100&amp;More - Data Manipulation and Cleaning</title>
      <link href="/2019/04/06/DS100%20-%20Data%20Manipulation%20and%20Cleaning/"/>
      <url>/2019/04/06/DS100%20-%20Data%20Manipulation%20and%20Cleaning/</url>
      
        <content type="html"><![CDATA[<p>This is the first section for my study note: DS100&amp;More, which records some useful points in data ETL(Extract, Transform and Loaded), visualisation, and modelling. The motivation is to keep a record of my second learning of UC Berkeley DS100 course:  Principles and Techniques of Data Science. By learning relative knowledge, reader should be familiar with some python tools that are in use now, as well as making some inferences from data by hand.</p><p>Please contact me at <a href="mailto:zr116@ic.ac.uk" target="_blank" rel="noopener">zr116@ic.ac.uk</a> for any correction and improvement, general discussion on data science is also welcome.</p><h3 id="Today‚Äôs-Objectives"><a href="#Today‚Äôs-Objectives" class="headerlink" title="Today‚Äôs Objectives"></a>Today‚Äôs Objectives</h3><ol><li>EDA(Exploratory Data Analysis)</li><li>Use pyhton Pandas to do EDA</li></ol><h3 id="Tabular-Data"><a href="#Tabular-Data" class="headerlink" title="Tabular Data"></a>Tabular Data</h3><p><a href="https://www.textbook.ds100.org/ch/03/pandas_intro.html" target="_blank" rel="noopener">DS100 Textbook: Tabular Data</a></p><p>The first thing to do when we have a new data set in hand is to look at its inside. Data can be structured in different ways:<br>1.1 Comma-Separated Values (CSV)<br>[example]<br>1.2 Tab-Separated Values (TSV)<br>2.1 JavaScript Object Format (JSON)<br>[curly brackets]<br>3.1 eXtensible Markup Language (XML)<br>[how websites store info]<br>3.2 HyperText Markup Language (HTML)</p><blockquote><p>CSV and TSV data is structured in a tabular structure, in rows and columns.<br>JSON, XML, HTML data has a hierarchical structure, like a tree.<br>(see Hokodo intern note for JSON processing methods)</p></blockquote><p>Check file format:</p><ol><li>right click and view profile</li><li>fancier way:<br>use command-line interface (CLI) tools/terminal<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls</span><br></pre></td></tr></table></figure></li></ol><p>or if you are in Jupyter notebook:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br></pre></td></tr></table></figure></p><blockquote><p>!+command line in Jupyter</p></blockquote><p>Although all these data formats are often used, CSV is often the easiest and most frequently used structure.</p><p>Before reading data, need to figure out how much memory it is going to take.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!ls -l -h folder</span><br><span class="line">!du -h folder</span><br><span class="line">#specific file usage</span><br><span class="line">!du -sh folder/*</span><br></pre></td></tr></table></figure><p>As a rule of thumb, reading in a file of x GB requires 4x GB memory. <code>Pandas</code> needs 2x but has to share memory with other programs.</p><blockquote><p><strong>Memory Overhead</strong><br>Note that memory is shared by all programs running on a computer, including the operating system, web browsers, and yes, Jupyter notebook itself. A computer with 4 GiB total RAM might have only 1 GiB available RAM with many applications running. With 1 GiB available RAM, it is unlikely that <code>pandas</code> will be able to read in a 1 GiB file.</p></blockquote><ul><li>To view CSV data in python:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">data = pd.read_csv(&apos;&apos;)</span><br></pre></td></tr></table></figure><ul><li>How many rows and columns:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.shape</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>Check the size of data before inpecting it to avoid printing too many columns, which is not helpful to getting to konw the data.</p></blockquote><ul><li><p>Total size:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.size</span><br></pre></td></tr></table></figure></li><li><p>Inspecting important statistical values of numerical variables in a data set:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.describe()</span><br></pre></td></tr></table></figure></li></ul><p>This returns count, mean, standard deviation, min, max, and quartiles. <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html" target="_blank" rel="noopener">Details</a></p><ul><li><p>inspect first/last several rows(default 5)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.head(5)#first 5</span><br><span class="line">data.tail(3)#last 3</span><br></pre></td></tr></table></figure></li><li><p>other common command</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#set &apos;col_name&apos; column as index column, in_place replace the old index column</span><br><span class="line">data.set_index(&apos;col_name&apos;, in_place = True)</span><br><span class="line"></span><br><span class="line">#list all column names</span><br><span class="line">data.columns</span><br><span class="line"></span><br><span class="line">#selection</span><br><span class="line">##select entries in column &apos;col1&apos;</span><br><span class="line">data[&apos;col1&apos;]</span><br><span class="line">##select two columns: col1, col2</span><br><span class="line">data[[&apos;col1&apos;,&apos;col2&apos;]]</span><br><span class="line">##make selected part into a DataFrame</span><br><span class="line">data[[&apos;col1&apos;,&apos;col2&apos;]].to_frame()</span><br><span class="line">##select rows that has val1 in col1</span><br><span class="line">data[data[&apos;col1&apos;]==val1]</span><br><span class="line"></span><br><span class="line">#column manipulation</span><br><span class="line">##count number of each value in a column</span><br><span class="line">data[&apos;col1&apos;].value_count()</span><br><span class="line">##print unique values in a column</span><br><span class="line">data[&apos;col1&apos;].unique()</span><br><span class="line">##statistical values of a column</span><br><span class="line">data[&apos;col1&apos;].min()</span><br><span class="line">data[&apos;col1&apos;].max()</span><br><span class="line">data[&apos;col1&apos;].median()</span><br><span class="line"></span><br><span class="line">#locator</span><br><span class="line">data.loc[row_list,col_list]</span><br><span class="line">data.iloc[[row_index],[column_index]]</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Study Notes </category>
          
          <category> DS Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Science </tag>
            
            <tag> Python </tag>
            
            <tag> Pandas </tag>
            
            <tag> Data Cleaning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Parallel Computing with Fortran, OpenMP and MPI</title>
      <link href="/2018/10/27/Parallel%20computing%20with%20Fortran,%20OpenMP%20and%20MPI/"/>
      <url>/2018/10/27/Parallel%20computing%20with%20Fortran,%20OpenMP%20and%20MPI/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> CS projects </category>
          
          <category> M3C@imperial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer science </tag>
            
            <tag> Parallel computing </tag>
            
            <tag> Fortran </tag>
            
            <tag> OpenMP </tag>
            
            <tag> MPI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stochastic Differential Equation simulations and uniform peacock problem</title>
      <link href="/2018/09/27/Stochastic%20Differential%20Equation%20simulations%20and%20uniform%20peacock%20problem/"/>
      <url>/2018/09/27/Stochastic%20Differential%20Equation%20simulations%20and%20uniform%20peacock%20problem/</url>
      
        <content type="html"><![CDATA[<p>In this report we will discuss stochastic differential equations and simulations of their roots. After that, a special SDE equation will be introduced and simulated using both Euler and Milstein method.</p><p>Find full report <a href="/files/UROP.pdf">here</a>.</p>]]></content>
      
      
      <categories>
          
          <category> Research projects </category>
          
          <category> UROP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Math </tag>
            
            <tag> Stochastic Differential Equations </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lagrangian Mechanics and Rigid Body Motion</title>
      <link href="/2018/06/20/Lagrangian%20Mechanics%20and%20Heavy-top%20Motion/"/>
      <url>/2018/06/20/Lagrangian%20Mechanics%20and%20Heavy-top%20Motion/</url>
      
        <content type="html"><![CDATA[<p>This is a group research project I did in my second year at Imperial College London as a math undergraduate student. I cooperated with the team in explaining and proving basic Lagrangian Mechanics and the investigation into the heavy-top problems.</p><p>Here is an abstract of what this project is about. The full report reveals more about the proof, examples and Matlab code.</p><blockquote><p>In our report we will discuss Lagrangian Mechanics and the Motion of Rigid Bodies. La- grangian Mechanics is a reformulation of Classical Mechanics, first introduced by the famous mathematician Joseph-Louis Lagrange, in 1788. We shall discuss the uses of Lagrangian Me- chanics and include two examples - the Spherical Pendulum and the Double Pendulum. In each case we will derive the equations of motion, and then try to solve these numerically and/or analytically. We will investigate the effect of removing the gravitational field (in the case of the Spherical Pendulum) and discuss any links between the two, as well as any implications of the solutions. <br><br>A rigid body is a collection of N points such that the distance between any two of them is fixed regardless of any external forces they are subject to. We shall look at the kinematics, the Inertia Tensor and Euler‚Äôs Equation and use this to explain about the dynamical stability of rigid bodies. Symmetric tops are the main example that we will investigate and discuss. We will look into the precession rate and the spinning rate and discuss two examples, Feynman‚Äôs wobbling plate and the hula hoop. A more complicated rigid body we shall then explore is the heavy symmetric top, in which we take into account the forces exerted by a gravitational field.</p></blockquote><p>Full report is <a href="/files/M2R.pdf">here</a>.</p>]]></content>
      
      
      <categories>
          
          <category> Research projects </category>
          
          <category> M2R </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Math </tag>
            
            <tag> Mechanics </tag>
            
            <tag> Matlab </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
